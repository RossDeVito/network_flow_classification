[0;31mInit signature:[0m
[0mattr[0m[0;34m.[0m[0mIntegratedGradients[0m[0;34m([0m[0;34m[0m
[0;34m[0m    [0mforward_func[0m[0;34m:[0m [0mCallable[0m[0;34m,[0m[0;34m[0m
[0;34m[0m    [0mmultiply_by_inputs[0m[0;34m:[0m [0mbool[0m [0;34m=[0m [0;32mTrue[0m[0;34m,[0m[0;34m[0m
[0;34m[0m[0;34m)[0m [0;34m->[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
[0;31mDocstring:[0m     
Integrated Gradients is an axiomatic model interpretability algorithm that
assigns an importance score to each input feature by approximating the
integral of gradients of the model's output with respect to the inputs
along the path (straight line) from given baselines / references to inputs.

Baselines can be provided as input arguments to attribute method.
To approximate the integral we can choose to use either a variant of
Riemann sum or Gauss-Legendre quadrature rule.

More details regarding the integrated gradients method can be found in the
original paper:
https://arxiv.org/abs/1703.01365
[0;31mInit docstring:[0m
Args:

    forward_func (callable):  The forward function of the model or any
            modification of it
    multiply_by_inputs (bool, optional): Indicates whether to factor
            model inputs' multiplier in the final attribution scores.
            In the literature this is also known as local vs global
            attribution. If inputs' multiplier isn't factored in,
            then that type of attribution method is also called local
            attribution. If it is, then that type of attribution
            method is called global.
            More detailed can be found here:
            https://arxiv.org/abs/1711.06104

            In case of integrated gradients, if `multiply_by_inputs`
            is set to True, final sensitivity scores are being multiplied by
            (inputs - baselines).
[0;31mFile:[0m           ~/anaconda3/envs/net/lib/python3.9/site-packages/captum/attr/_core/integrated_gradients.py
[0;31mType:[0m           type
[0;31mSubclasses:[0m     
